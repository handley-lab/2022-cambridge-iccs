{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250d8396",
   "metadata": {},
   "source": [
    "# Example 1: Introduction to regression\n",
    "\n",
    " This notebook is designed to introduce the basic concepts of training models\n",
    " using the numpy and scipy libraries alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c37d2",
   "metadata": {},
   "source": [
    "Import the key libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e0b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424175af",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = np.loadtxt('example_1_data.csv').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d833ad",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd316ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(data_x, data_y, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619111a3",
   "metadata": {},
   "source": [
    "Define our model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, theta):\n",
    "    i = np.arange(len(theta))\n",
    "    x = np.array(x)\n",
    "    return (theta * x[..., None]**i).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c27db",
   "metadata": {},
   "source": [
    "- **Q1**: What does the `f` define?\n",
    "- **Q2**: What does `None` slicing do?\n",
    "- **Q3**: Why choose `...` rather than `:`?\n",
    "- **Q4**: Why is `axis=-1` essential to the above?\n",
    "- **(Optional exercise)**: Are you able to extend the above to `theta`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4aec8",
   "metadata": {},
   "source": [
    "Now fit the model to the data by minimising a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17983916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def loss(theta):\n",
    "    return ((f(data_x, theta) - data_y)**2).sum()\n",
    "\n",
    "theta0 = np.zeros(4)\n",
    "sol = minimize(loss, theta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335a55c",
   "metadata": {},
   "source": [
    "- **Q5**: \n",
    " Plot the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c73f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = sol.x\n",
    "plt.plot(data_x, data_y, '.')\n",
    "x = np.linspace(-1,3,1001)\n",
    "plt.plot(x, f(x, theta));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8235b49",
   "metadata": {},
   "source": [
    "Consider models of increasing complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acabf946",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_x, data_y, '.')\n",
    "for n in [0, 1, 2, 3, 4]:\n",
    "    sol = minimize(loss, np.zeros(n+1))\n",
    "    theta = sol.x\n",
    "    plt.plot(x, f(x, theta));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475404b3",
   "metadata": {},
   "source": [
    "**Q6**: Where is the quadratic solution? (hint, try adding a legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347e784",
   "metadata": {},
   "source": [
    "**Q7:** What's goes wrong with the following?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb292cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_x, data_y, '.')\n",
    "sol = minimize(loss, np.zeros(10))\n",
    "theta = sol.x\n",
    "plt.plot(x, f(x, theta));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac35322",
   "metadata": {},
   "source": [
    "**Q8:** Plot the loss as a function of `n`\n",
    " **Q9:** How could we go about choosing n? Programme something if you have time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc921dba",
   "metadata": {},
   "source": [
    "This dataset was generated with the code below.\n",
    " \n",
    " Try modifying the data and see how these observations change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58827b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.uniform(-1,3,50)\n",
    "y = 1 + (x-1)**3 -(x-1)\n",
    "y += np.random.randn(len(y))\n",
    "np.savetxt('example_1_data.csv', np.array([x, y]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dcd524",
   "metadata": {},
   "source": [
    "# BREAK\n",
    " \n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d0921",
   "metadata": {},
   "source": [
    "# Example 2: Machine Learning with scikit learn\n",
    " \n",
    " https://www.kaggle.com/datasets/emmanuelfwerr/london-weather-data\n",
    "\n",
    " Can we predict the rain in London?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b6541",
   "metadata": {},
   "source": [
    "## Exploring the data and showcasing pandas\n",
    " First import the data raw as downloaded from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b821fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('london_weather.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28cc93",
   "metadata": {},
   "source": [
    "We can also get some broad info, which is useful to show us how many non-null values there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d560c61c",
   "metadata": {},
   "source": [
    "The pandas describe method is useful for getting a quick overview of the stastistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5385b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6165d9aa",
   "metadata": {},
   "source": [
    "Note that the counts don't agree -- this is due to missing data, mostly in the snow_depth\n",
    " Let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b9beed",
   "metadata": {},
   "source": [
    "We could also have also used an imputer to fill it with the median\n",
    " **(Optional exercise)**\n",
    "\n",
    "`from sklearn.impute import SimpleImputer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab832293",
   "metadata": {},
   "source": [
    "The pandas hist methoda is a useful tool for showing an overview of the shape of the data (useful for eyeballing distributions & scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfcdab6",
   "metadata": {},
   "source": [
    "Now look into correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7314bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "corr_matrix['sunshine'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea31bae",
   "metadata": {},
   "source": [
    "Unsurprisingly sunshine correlates with temperatures and pressures, and\n",
    " anticorrelates with rain and cloud cover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e08dc6",
   "metadata": {},
   "source": [
    "We can also plot the full scatter matrix, which is much more involved,\n",
    " showing the scatter plots of pairwise parameters. Note that down the\n",
    " diagonal it shows the histograms as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a383a",
   "metadata": {},
   "source": [
    "It's also relatively easy to define new variables in pandas, e.g. an integer detailing the month. Here we use the `to_datetime` functionality to convert to a column of datetimes, and the pandas `.dt.month` operation to extract the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = pd.to_datetime(data.date, format='%Y%m%d').dt.month\n",
    "corr_matrix = data.corr()\n",
    "corr_matrix['precipitation'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72264e53",
   "metadata": {},
   "source": [
    "Surprising -- quite weak correlations... but!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf99ef",
   "metadata": {},
   "source": [
    "It's a non-linear relationship with month. Let's drop it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6807f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns='month', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1202b3",
   "metadata": {},
   "source": [
    "## Showcasing scikit-learn\n",
    " Let's start by trying to predict the amount of temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['sunshine']\n",
    "X = data.drop(columns=['sunshine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e5a3f",
   "metadata": {},
   "source": [
    "Split into training and testing data using the scikit learn function\n",
    "\n",
    " **(Optional Exercise)**: Look into using sklearn.model_selection.StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69da515",
   "metadata": {},
   "source": [
    "Now we need to standardise the data by zero meaning and scaling to unit variance. Other options include `MaxMinScaler`, as well as custom transformations. Look into `sklearn.compose.CoumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = StandardScaler()\n",
    "pipeline.fit(X_train)\n",
    "X_trans = pipeline.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010f859",
   "metadata": {},
   "source": [
    "Could also run `X_trans = pipeline.fit_transform(X_train)`  to 'fit' the pipeline and and scale the data simultaneously\n",
    "\n",
    " Now let's train a model!\n",
    "\n",
    " Start with something embarrassingly simple -- linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_trans, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637faac",
   "metadata": {},
   "source": [
    "Done! Now let's test the predictions on the _training_ set (i.e. how well it has done in-sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "y_pred = lin_reg.predict(X_trans)\n",
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851ce5e",
   "metadata": {},
   "source": [
    "Well that's only OK when compared to the spread in y itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761048db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90327544",
   "metadata": {},
   "source": [
    "A scatter plot of predictions is also not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e002a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_train, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343696c",
   "metadata": {},
   "source": [
    "We're clearly doing something though!\n",
    " Let's try something different -- a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_trans, y_train)\n",
    "y_pred = tree_reg.predict(X_trans)\n",
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa971be4",
   "metadata": {},
   "source": [
    "Well that seem fishy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175821b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbc20d",
   "metadata": {},
   "source": [
    "This is almost certainly an example of overfitting. At this point we would be tempted to look at the testing data, but it is better to hold that back until later.\n",
    " Let's try some cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, X_trans, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02bd32d",
   "metadata": {},
   "source": [
    "So out-of-sample apparently worse than the linear regression -- let's check for sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368894c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lin_reg, X_trans, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adb9dc",
   "metadata": {},
   "source": [
    "Let's try a `RandomForestRegressor` --  tthis is effectively a stack of decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(X_trans, y_train)\n",
    "y_pred = forest_reg.predict(X_trans)\n",
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b2b21",
   "metadata": {},
   "source": [
    "Seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ad42d",
   "metadata": {},
   "source": [
    "Out of sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(forest_reg, X_trans, y_train, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7422bf",
   "metadata": {},
   "source": [
    "So slightly better than the linear regression, and clearly overfit in sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d490f2",
   "metadata": {},
   "source": [
    "To tune the hyperparameters we can run a grid search over the hyperparameters for the `RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "        {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "        {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "        ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_trans, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39f4a2",
   "metadata": {},
   "source": [
    "We can get the best params as an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e80272",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016831a",
   "metadata": {},
   "source": [
    "and the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c099c3c",
   "metadata": {},
   "source": [
    "As well as the total grid of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mean_score, params in zip(grid_search.cv_results_[\"mean_test_score\"], grid_search.cv_results_[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d5419",
   "metadata": {},
   "source": [
    "Also useful to look at which features are the most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, importance in zip(X_train.columns, grid_search.best_estimator_.feature_importances_):\n",
    "    print(feature, importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6900ccd",
   "metadata": {},
   "source": [
    "global radiation and cloud cover seem to be the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c544e30",
   "metadata": {},
   "source": [
    "Now try it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_trans = pipeline.transform(X_test)\n",
    "y_pred = final_model.predict(X_trans)\n",
    "\n",
    "plt.scatter(y_test, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34ad87",
   "metadata": {},
   "source": [
    "Let's compare to lineara regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dbf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, lin_reg.predict(X_trans));\n",
    "plt.scatter(y_test, tree_reg.predict(X_trans));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde2ab0",
   "metadata": {},
   "source": [
    "Interestingly it seems that the main reason the tree does better is dealing with those outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60015222",
   "metadata": {},
   "source": [
    "Exercises:\n",
    " - **Q10** How do the above change if we try to predict rainfall? temperature?\n",
    " - **Q11** Are any other of the out-of-the-box methods any better?\n",
    " - **Q12** Do these results change if you add in any new features e.g. month/year?\n",
    " - **Q13** Try this analysis out using other datasets from kaggle?\n",
    " - **Q14** How do the results change if we remove those outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f798263",
   "metadata": {},
   "source": [
    "# BREAK\n",
    " \n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d945286c",
   "metadata": {},
   "source": [
    "# Example 3: Neural network training\n",
    "\n",
    " ## Keras TensorFlow sequential API\n",
    " Let's specify an anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(X.shape[-1]))\n",
    "model.add(keras.layers.Dense(30, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(30, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(30, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation=\"softplus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9889173",
   "metadata": {},
   "source": [
    "Alternatively we can do it in one go\n",
    "```\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(X.shape[-1]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"softplus\") \n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bb401",
   "metadata": {},
   "source": [
    "Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30fd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729595e3",
   "metadata": {},
   "source": [
    "Specify the model training procedures with 'compilation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eba5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6b046",
   "metadata": {},
   "source": [
    "Split into validation and testing using sklearn's `test_train_split` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans_, X_valid, y_train_, y_valid = train_test_split(pipeline.transform(X_train), y_train, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a637053",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_trans_, y_train_, epochs=50, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f24ca8",
   "metadata": {},
   "source": [
    "Plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f0a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7da30",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, lin_reg.predict(pipeline.transform(X_test)));\n",
    "plt.scatter(y_test, model.predict(pipeline.transform(X_test)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1edb2c",
   "metadata": {},
   "source": [
    "## Keras TensorFlow functional API\n",
    " Now let's look at the functional API, implemented for a non-sequential 'wide and deep' network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.layers.Input(X.shape[-1])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input, hidden2])\n",
    "output = keras.layers.Dense(1, activation=\"softplus\")(concat)\n",
    "\n",
    "model = keras.models.Model(inputs=[input], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9b464",
   "metadata": {},
   "source": [
    "Now let's fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd29983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_trans_, y_train_, epochs=50, validation_data=(X_valid, y_valid))\n",
    "            \n",
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26459307",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a411ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, lin_reg.predict(pipeline.transform(X_test)));\n",
    "plt.scatter(y_test, model.predict(pipeline.transform(X_test)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f488f8d",
   "metadata": {},
   "source": [
    "## Keras TensorFlow Subclssing API\n",
    " This is useful for dynamic models. Here we implement the same 'wide and deep' network as above, but in principle this could change dynamically according to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.output_ = keras.layers.Dense(1, activation=\"softplus\")\n",
    "\n",
    "    def call(self, input):\n",
    "        hidden1 = self.hidden1(input)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.Concatenate()([input, hidden2])\n",
    "        output = self.output_(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948828b",
   "metadata": {},
   "source": [
    "We define `model_` rather than `model` purely because we wish to return to the previous model earlier (subclassed models are more restrictive than functional or sequential models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = WideAndDeepModel()\n",
    "model_.compile(loss=\"mean_squared_error\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "history = model_.fit(X_trans_, y_train_, epochs=50, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a6aa6",
   "metadata": {},
   "source": [
    "We can inspect any weights we choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917fc55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.get_layer('dense_4').get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3dc1d",
   "metadata": {},
   "source": [
    "Saving and restoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9be59",
   "metadata": {},
   "source": [
    "Tasks\n",
    " - **Q15** Make a more systematic comparison of all the methods losses\n",
    " - **Q15** Explore changing the neural network architectures/training mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83918b34",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed196c04",
   "metadata": {},
   "source": [
    "Define a sequential model for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b41080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X.shape[-1],30),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(30,30),\n",
    "    torch.nn.ReLU(), \n",
    "    torch.nn.Linear(30,1),\n",
    "    torch.nn.Softplus(), \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201caac",
   "metadata": {},
   "source": [
    "Define a mean square error loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca25e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f438fcf",
   "metadata": {},
   "source": [
    "Define a stochastic gradient descent optimiser with learning rate 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab855ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684249b",
   "metadata": {},
   "source": [
    "(slightly verbose) mechanism for passing training data into a format that pytorch can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add6562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_trans_).float(), torch.tensor(y_train_.values).float()), batch_size=32)\n",
    "valid_dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_valid).float(), torch.tensor(y_valid.values).float()), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bebe9a5",
   "metadata": {},
   "source": [
    "Very neat package for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c16610",
   "metadata": {},
   "source": [
    "The meat of the code -- train! (will discuss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm.tqdm(np.arange(50))\n",
    "for _ in pbar:\n",
    "\n",
    "    sum_train_loss = 0\n",
    "    for x_batch, y_batch in train_dataloader:\n",
    "        pred = model(x_batch)[:, 0]\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        sum_train_loss += loss.item()\n",
    "\n",
    "    sum_valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_dataloader:\n",
    "            pred = model(x_batch)[:, 0] \n",
    "            loss = loss_fn(pred, y_batch)\n",
    "            sum_valid_loss += loss.item()\n",
    "\n",
    "    pbar.set_description(\"Loss: (%.2f, %.2f)\" % (sum_train_loss/len(train_dataloader),\n",
    "                                                 sum_valid_loss/len(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d373d1",
   "metadata": {},
   "source": [
    "Plot the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(torch.from_numpy(pipeline.transform(X_test)).float()).detach().numpy()\n",
    "plt.scatter(y_test, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bef21",
   "metadata": {},
   "source": [
    "Tasks\n",
    " - **Q16** Compare and contrast the pytorch approach vs the keras approach. Which do you prefer?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
